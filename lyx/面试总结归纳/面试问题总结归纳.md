1、基础题方面不太熟练（特别是集合等必回的题）

2、mybatis方面不太熟练



锁

锁的有效期并不能支持到业务完成，A业务还没完成锁过期了，然后B业务能抢到锁，并且执行跟A业务一样的业务，就会导致问题。

要加看门狗，来延长有效期。

防止自己误删了别人加的锁：

在value中添加自己的唯一标识



使用分段锁来提高锁的性能

例如：1~100个商品

id_1:1~10

id_2:10~20

.....

id_10:90~100

分段的去处理



红锁

主要出现在分布式锁的场景中，当master设置了一个锁成功之后，给代码响应加锁成功，并且数据没有同步到slave从服务器中，master非常不巧就挂了，就把slave选举成为主，突然另外一个并发请求来了，因为没有同步到数据，新的并发就会加上锁，就会导致前面的业务和后面的业务在同一个方法里面执行同一个业务逻辑，就会出问题。



所以要让集群之间不做数据同步，超过一半的redis加锁成功那么就认为是成功的。



红锁的redis挂了之后不要立马重启！ 隔一天或者两天重启，这样可以拖到锁过期，或者业务执行完



STW当JVM和看门狗停滞了，被冻住了，他就不会再向redis里面续期了，这样新请求过来又会加锁成功，出现问题







具体问题：

1. equles和==的区别
2. 集合相关问题：java有哪些集合，集合之间的区别，map有多少种，哪些map是线程安全的，map是如何遍历的，LinkedHashMap
3. 线程和进程的区别
4. mybatis的常用标签，mybatis模糊查询是如何写的，mybatis如何实现一对多的关系
5. Spring的数据结构是堆还是队列的形式，Spring定时任务的实现方式
6. SpringBoot的加载顺序、配置文件的加载顺序
7. 聚合函数分组，什么时候group by
8. mybatis的缓存和二级缓存
9. 如何将集合中的某个元素删除
10. redis 的删除机制有哪些，redis的主从复制模式，redis和mysql如何实现双写的
11. mysql调优的方式有哪些，你是怎么调优的
12. mysql索引在什么情况下会失效
13. 自增id和uuid 的区别
14. 线程池、你对线程池的理解
15. Websocket
16. mysql去重的几种方式
17. Dubbo
18. mybatis 的分页实现方式，一次分页会执行几次sql查询
19. SpringBoot的starter启动原理
20. kafka 的ACK确认机制
21. zookeeper对于kafka的意义是什么样的？



### 蚂蚁金服一面：

先自我介绍，讲讲自己基础掌握情况，以及项目经历平时会用到哪些数据结构？
链表和数组的优缺点？
解决hash冲突的方法有哪些？
讲讲自己对HashMap的理解，以及和Weakhashmap的区别？
你刚才讲的是JDK1.7版本的实现，知道JDK1.8做了哪些改动么？
你们在微服务中用RPC通信还是REST？
RPC和HTTP的关系是什么？
谈谈什么是HTTP的长连接和短连接？
TCP的三次握手和四次挥手，以及为什么要三次握手，而不是二次？
TCP 有哪些状态，相应状态的含义?并发包中锁的实现底层（对AQS的理解）？
乐观锁和悲观锁的理解及如何实现，有哪些实现方式？
SynchronizedMap和ConcurrentHashMap有什么区别？如何使用阻塞队列实现一个生产者和消费者模型？
简述一下Java 垃圾回收机制？
如何判断一个对象是否存活？
什么是tomcat类加载机制？
类加载器双亲委派模型机制？
让你评价一下你自己？

### 蚂蚁金服二面:

扯了下项目、讲一下项目经历redis的底层数据结构了解多少？
知道动态字符串sds的优缺点么(redis底层数据结构之一)？
redis的单线程特性有什么优缺点？
用过 Redis 的哪些数据结构, 分别用在什么场景?怎么解决缓存击穿问题的？
Hytrix的隔离机制有哪些？
Hytrix常见配置是哪些？
做过哪些调优？
JVM调优、数据库调优都行！给了个场景，问你怎么调？
蚂蚁金服三面：
依然是介绍自己数据库的高可用架构是怎么样的？
如何保证数据库主从一致性？
知道mysql的索引算法吗？
为什么mongodb的索引用了B树，而mysql用B+树？
用mysql过程中，有遇到什么问题么？
生产用的是哪种事务隔离级别，为什么？
谈一谈你对微服务架构的理解？
用过哪些RPC框架，讲讲他们优缺点？



### 跨越速运1面：

Redis分布式锁的实现
Redis还有哪些应用场景使用
Redis如何防止自己的锁被别人删了？怎么实现？
Redis如何实现排行榜？
ZSET的使用？
如何防止接口的幂等
Spring在运行过程中如何修改对象的属性？
了解BeanPostProcessor吗？实现这个接口可以干嘛？
Java线程池大小设置多少合适？为什么要这样设置？
Java线程池的执行流程是怎么样的？
用哪个对象来保证线程之间的通信
Kafka如何保证数据的不丢失？kafka是的数据存储为什么能够那么快？







### kafka面试问题

**consumer** **是推还是拉？**

Kafka 最初考虑的问题是，customer 应该从 brokes 拉取消息还是 brokers 将消息推送到 consumer，

也就是 pull 还 push。在这方面，Kafka 遵循了一种大部分消息系统共同的传统的设计：producer 将消

息推送到 broker，consumer 从broker 拉取消息。

一些消息系统比如 Scribe 和 Apache Flume 采用了 push 模式，将消息推送到下游的 consumer。这样

做有好处也有坏处：由 broker 决定消息推送的速率，对于不同消费速率的 consumer 就不太好处理

了。消息系统都致力于让 consumer 以最大的速率最快速的消费消息，但不幸的是，push 模式下，当

broker 推送的速率远大于 consumer 消费的速率时，consumer 恐怕就要崩溃了。最终 Kafka 还是选

取了传统的 pull 模式。Pull 模式的另外一个好处是 consumer 可以自主决定是否批量的从 broker 拉取数据 。Push 模式必须

在不知道下游 consumer 消费能力和消费策略的情况下决定是立即推送每条消息还是缓存之后批量推

送。如果为了避免 consumer 崩溃而采用较低的推送速率，将可能导致一次只推送较少的消息而造成浪

费。Pull 模式下，consumer 就可以根据自己的消费能力去决定这些策略。

Pull 有个缺点是，如果 broker 没有可供消费的消息，将导致 consumer 不断在循环中轮询，直到新消

息到 t 达。为了避免这点，Kafka 有个参数可以让 consumer阻塞知道新消息到达(当然也可以阻塞知道

消息的数量达到某个特定的量这样就可以批量发送)。 



**4、讲讲kafka维护消费状态跟踪的方法**

大部分消息系统在 broker 端的维护消息被消费的记录：一个消息被分发到consumer 后 broker 就马上

进行标记或者等待 customer 的通知后进行标记。这样也可以在消息在消费后立马就删除以减少空间占

用。



### **kafka的刷盘机制**

**log.flush.interval.messages： 在将消息刷新到磁盘之前，在日志分区上积累的消息数量**

**log.flush.interval.ms： 在刷新到磁盘之前，任何topic中的消息保留在内存中的最长时间**

**log.flush.scheduler.interval.ms： 日志刷新器检查是否需要将所有日志刷新到磁盘的频率**



首先需要配置Broker的副本同步配置

同步刷盘和异步刷盘

同步刷盘：可以保证消息一定不会丢失

异步刷盘：消息有一定丢失的概率



生产者消息同步投递、生产者消息的确认机制。

消息ack确认机制要配置为-1，



producer回显记录日志，再将消息投递



Consumer消费者不能设置自动提交，要自己手动提交，如果数据没有消费成功的话我们要去重试，保证消息不丢失

1、服务器端设置为同步刷盘

2、生产者要设置成同步的投递消息

3、ACK确认要设置为手动提交



kafka中一个topic会被分割成多个partition分区，当用户查看创建的一个partition的时候，可以看到里面有三个文件：log，index，timeindex文件，里面存储的都是二进制格式的数据，log文件存的是batchRecords消息内容，另外两个分别存储的是一些索引信息。这三个文件共同组成了一个segment。可以通过配置设置segment的大小，当写入数据达到阈值的时候，就会创建一个新的segment。根据log、index、timeindex就可以基于offset找到对应的message



**但是这样会不会有什么问题呢？**

如果一条消息发送出去之后就立即被标记为消费过的，旦 consumer 处理消息时失败了（比如程序崩溃）

消息就丢失了。为了解决这个问题，很多消息系统提供了另外一个个功能：当消息被发送出去之后仅仅被标记为已发送状态，当接到 consumer 已经消费成功的通知后才标记为已被消费的状态。这虽然解决了消息丢失的问题，但产生了新问题，首先如果 consumer处理消息成功了但是向 broker 发送响应时失败了，这条消息将被消费两次。第二个问题时，broker 必须维护每条消息的状态，并且每次都要先锁住消息然后更改状态然后释放锁。这样麻烦又来了，且不说要维护大量的状态数据，比如如果消息发送出去但没有收到消费成功的通知，这条消息将一直处于被锁定的状态，Kafka 采用了不同的策略。Topic 被分成了若干分区，每个分区在同一时间只被一个 consumer 消费。这意味着每个分区被消费的消息在日志中的位置仅仅是一个简单的整数：offset。这样就很容易标记每个分区消费状态就很容易了，仅仅需要一个整数而已。这样消费状态的跟踪就很简单了。这带来了另外一个好处：consumer 可以把 offset 调成一个较老的值，去重新消费老的消息。这对传统的消息系统来说看起来有些不可思议，但确实是非常有用的，谁规定了一条消息只能被消费一次呢？



**java序列化**

代表着该实例在内存中可以永久的保存和传输。





### 基础



#### HashMap是线程安全的吗

HashMap在涉及的时候是针对单线程环境下使用的，所以他不是线程安全的。

多线程并发下使用ConcurrentHashMap，他的底层实现原理分为两种情况分析：

1、在jdk1.7的时候底层使用的是数组加链表的结构实现，使用了一种分段锁来保证线程安全，将数组分为了16段，给每个segment来分配一把锁，在读每个segment的时候要先获取对应的锁。所以他最多能有16个线程来并发操作这个数组。

2、jdk1.8之后引入了红黑树的结果，在并发处理方面去除了分段锁，使用了CAS自旋锁加synchronized关键字来更加细粒度哈希桶的实现。在写入键值对的时候可以锁住哈希桶的头结点，这样不会影响别的桶的写入，提高并发能力。



#### 是否可以使用ReentrantLock锁来实现。

是可以的，但是使用synchronized关键字会更好，因为随着jdk版本的提升对synchronized关键字进行了优化，引入了偏向锁、轻量锁、重量级锁。



#### synchronized关键字介绍

默认采用的是偏向锁，在运行的过程中由一个线程去获取锁，java对象会记录线程的id，下次再获取 锁的时候只要去比线程的id就行了，在运行过程中如果出现第二个线程去请求synchronized锁的时候，如果没有发生并发竞争锁的情况下他会升级成轻量级锁。如果出现两个线程一起争抢锁的话就会升级成重量级锁，这个时候就只能由一个线程去获取到锁，另外一个线程会阻塞。要等待第一个线程释放锁之后才能拿到锁。























